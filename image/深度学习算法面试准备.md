# 深度学习算法面试准备

## **资料：**
[算法工程师面试题【集锦cv/ml/dl】](https://blog.csdn.net/weixin_41923961/article/details/82770445)

[ 计算机视觉面试之语义分割](https://www.jianshu.com/p/8838ed42f0b4)

[ 深度学习算法面试题总结](https://blog.csdn.net/attitude_yu/article/details/80963323)
## **问题与答案：**

### **一、深度学习方面**

#### **1.max pool层怎么做的？**

[CNN卷积神经网络算法之Max pooling池化操作学习](https://www.cnblogs.com/python-frog/p/9380290.html)

![](https://images2018.cnblogs.com/blog/1358931/201807/1358931-20180727233443645-658886916.png)

#### **2.反卷积**

[图像卷积与反卷积](https://blog.csdn.net/fate_fjh/article/details/52882134)

![](https://img-blog.csdn.net/20161021154222794)

3.单个神经元是否线性可分（模式识别的概念，是否能用用线性函数将样本分类）？

是否线性可分是对于样本集的;线性可分是数据集合的性质，和分类器没啥关系。
可以通过线性函数分类的即为线性可分

#### **4.目标检测常用的网络,RCNN, SPP, Fast RCNN, Faster RCNN的区别？**

[RCNN,fast RCNN,faster RCNN区别](https://blog.csdn.net/qq_40239482/article/details/80306261)

RCNN
　　1. 在图像中确定约1000-2000个候选框 (使用选择性搜索)
　　2\. 每个候选框内图像块缩放至相同大小，并输入到CNN内进行特征提取 
　　3. 对候选框中提取出的特征，使用分类器判别是否属于一个特定类 
　　4. 对于属于某一特征的候选框，用回归器进一步调整其位置

Fast RCNN
　　1. 在图像中确定约1000-2000个候选框 (使用选择性搜索)
　　2. 对整张图片输进CNN，得到feature map
　　3. 找到每个候选框在feature map上的映射patch，将此patch作为每个候选框的卷积特征输入到SPP layer和之后的层
　　4. 对候选框中提取出的特征，使用分类器判别是否属于一个特定类 
　　5. 对于属于某一特征的候选框，用回归器进一步调整其位置

Faster RCNN
　　1. 对整张图片输进CNN，得到feature map
　　2. 卷积特征输入到RPN，得到候选框的特征信息
　　3. 对候选框中提取出的特征，使用分类器判别是否属于一个特定类 
　　4. 对于属于某一特征的候选框，用回归器进一步调整其位置

####** 5、SVM和softmax的区别？**

Svm具有附加稳定性，当样例满足边界条件时，该样例不会影响损失函数；而softmax将考虑所有的样例

#### **6、训练时，mini-batch与GPU的内存匹配?**

训练网络时的mini batch是由GPU的内存决定的。

#### **7、正则化：**

正则化表现的是对高维度W的惩罚力度，当正则化系数（lambda）很大时，使w变的非常小，最终的结果是函数变得非常平滑。正则化系数（lambda）越小，拟合程度越高，效果越好。

#### **8、当训练到最后，loss值很大，但精度在上升？**

说明loss变化很小，需要增大学习率

梯度爆炸（loss发散，出现nan）?

学习率很大，需要减小学习率

#### **9.优化策略的比较：**

[ 深度学习最全优化方法总结比较](https://www.sohu.com/a/148857062_494939)

SGD-->Momentum updata-->Nesterov Momentum updata-->AdaGrad update--> RMSProp update-->Adam update

以上都是一阶优化方法，对于二阶优化方法(BFGS和L-BFGS)，二阶优化方法不需要学习率这个参数，可以直接对目标进行优化。

SGD:根据梯度直接更新w

Momentum updata：不是通过计算得到的梯度直接更新w，而是增加一个变量V（定义为速度），改变了和梯度直接相关，再用V更新w

Nesterov Momentum updata:更新方式

AdaGrad update：每个参数自适应学习速率的方法（因为参数空间的每一维都有自己的学习速率，它会根据梯度的规模的大小动态变化）

长时间训练时，AdaGrad算法会发生什么？

根据更新公式，不断有正数加到cache中，更新步长会逐渐衰减到0，最后完全停止学习。1e-7：平滑因子，防止除数变成0

RMSProp update:解决了AdaGrad中会停止更新的问题

Adam update:

  adagrad记录的是梯度的二阶矩，并按指数和形式表示

Momentum的作用：稳定梯度的方向

**梯度下降法和牛顿法的优缺点？**

优点：梯度下降法：可用于数据量较大的情况
牛顿法：收敛速度更快；
缺点：梯度下降法：每一步可能不是向着最优解的方向；
牛顿法：每次迭代的时间长；需要计算一阶和二阶导数；


#### **10、定位和检测的区别：**

区别在于要找的目标的数量；

对于定位，图像中只有一个或一种对象，用框标出对象的位置
#### 
对于检测，图像中有多个目标或多种对象。

#### **11.数据不足时：**

数据增强、transfer learning（fine-tuning：根据数据集的大小，训练网络的最后一层或者最后几层）、修改网络

Fine-tuning:固定网络，即为学习率为0、需要训练的层的学习率比较高（原来训练好的网络的学习率的十分之一）、当预训练的层（中间层）需要改变时，学习率很小（如原学习率的一百分之一）

#### ** 11.1 解决训练样本类别不平衡问题？**

现象：训练样本中，正负样本数量的比例较大。

1. 过采样。增加正例样本数量，使得正负样本数量接近，然后再进行学习。

2. 欠采样。去除反例样本数量，使得正负样本数量接近，然后再进行学习。

3. 设置阈值。基于原始数据集学习，当使用已训练好的分类器进行预测时，将正负样本数量的比例作为阈值嵌入到决策过程中。



#### **12、语义分割（Semantic Segmentation）和实例分割（Instance Segmentation）**

语义分割-->操作像素，标记每个像素所属的标签à不关心具体的类，同一类目标标记为相同的像素

实例分割à 输出类别同时标记像素（同时检测并分割）-->关心目标的类，不同目标标记为不同的像素（同一类中的目标也标记为不同 的像素）

#### **13.L0、L1、L2正则化？**

L0范数：计算向量0元素的个数。

L1范数：计算向量中各元素绝对值之和。

L2范数：计算向量中各元素平方和的开方。

L0范数和L1范数目的是使参数稀疏化。L1范数比L0范数容易优化求解。

L2范数是防止过拟合，提高模型的泛化性能。


正则化就是抑制函数中的高维参数，防止过拟合。

#### **14.无监督学习方法有哪些？**

强化学习、K-means 聚类、自编码、受限波尔兹曼机

#### **15.空洞卷积(dilated convolution)的理解？**

基于FCN的语义分割问题中，需保持输入图像与输出特征图的size相同。

若使用池化层，则降低了特征图size,需在高层阶段使用上采样，由于池化会损失信息，所以此方法会影响导致精度降低；

若使用较小的卷积核尺寸，虽可以实现输入输出特征图的size相同，但输出特征图的各个节点感受野小；

若使用较大的卷积核尺寸，由于需增加特征图通道数，此方法会导致计算量较大；

所以，引入空洞卷积(dilatedconvolution),在卷积后的特征图上进行0填充扩大特征图size，这样既因为有卷积核增大感受野，也因为0填充保持计算点不变。

#### **16.增大感受野的方法？**

空洞卷积、池化操作、较大卷积核尺寸的卷积操作


#### **17. 各个激活函数的优缺点？**

Sigmoid激活函数 缺点：

1. 不是关于原点对称；

2. 需要计算exp

Tanh 激活函数 优点：

1. 关于原点对称

2. 比sigmoid梯度更新更快

ReLU激活函数 优点：

1. 神经元输出为正时，没有饱和区

2. 计算复杂度低，效率高

3. 在实际应用中，比sigmoid、tanh更新更快

4. 相比于sigmoid更加符合生物特性

ReLU激活函数 缺点：

1. 神经元输出为负时，进入了饱和区

2. 神经元的输出在非0中心

3. 使得数据存在Active ReLU、Dead ReLU(当wx+b<0时，将永远无法进行权值更新，此时的神经元将死掉)的问题

Leaky ReLU激活函数 优点：

1. 解决了ReLU激活函数Dead ReLU问题；

Maxout激活函数max(w1*x+b1,w2*x+b2)  缺点：

2. 参数较多；

#### **18. 神经网络的正则化方法？/过拟合的解决方法？**

数据增强(镜像对称、随机裁剪、旋转图像、剪切图像、局部弯曲图像、色彩转换)

early stopping(比较训练损失和验证损失曲线，验证损失最小即为最优迭代次数)

L2正则化(权重参数的平方和)

L1正则化(权重参数的绝对值之和)

dropout 正则化(设置keep_pro参数随机让当前层神经元失活)

#### **19. 目标检测领域的常见算法？**

两阶段检测器：R-CNN、Fast R-CNN、Faster R-CNN

单阶段检测器：YOLO、YOLO9000、SSD、DSSD、RetinaNet

#### **20.Batch Normalization如何实现？作用？**

实现过程

计算训练阶段mini_batch数量激活函数前结果的均值和方差，然后对其进行归一化，最后对其进行缩放和平移。

作用

1. 限制参数对隐层数据分布的影响，使其始终保持均值为0，方差为1的分布；

2. 削弱了前层参数和后层参数之间的联系，使得当前层稍稍独立于其他层，加快收敛速度；

3. 有轻微的正则化效果。


#### **21.梯度消失和梯度爆炸？**

原因：激活函数的选择。

梯度消失：令bias=0，则神经网络的输出结果等于各层权重参数的积再与输入数据集相乘，若参数值较小时，则权重参数呈指数级减小。或者是进入了激活函数的死区，导致梯度为0。

梯度爆炸：令bias=0，则神经网络的输出结果等于各层权重参数的积再与输入数据集相乘，若参数值较大时，则权重参数呈指数级增长。

#### **22.列举至少三种损失函数，写出数学表达式并简述各自优点**

https://blog.csdn.net/heyongluoyao8/article/details/52462400

0-1损失 感知机损失 绝对值损失

平方误差损失(线性回归)

对数损失(逻辑回归)

指数损失(Adaboost)

铰链损失(SVM)


#### **23.分类问题的评价标准**


准确率 = (TP+TN)/总样本数

精确率 = TP/(TP+FP) = 所有预测为正类样本中正类的概率

召回率 = TP/(TP+FN) = 所有真正类样本中正类的概率

2/调和平均值 = 1/精确率+1/召回率

P-R曲线：纵轴为精确率，横轴为召回率，基于平衡点(P=R)度量各个基分类器的优劣；

ROC曲线：纵轴为TPR，横轴为FPR

TPR = TP/(TP+FN)  FPR = FP/(FP+TN)

AUC:ROC曲线下的面积

mAP = 所有类别的AP之和/类别数量

P = (一张图片类别C识别正确数量)/(一张图片类别C的总数量)

AP = 每张图片的P之和/图片数量


#### **24.深度学习为什么在计算机视觉领域这么好**

以目标检测为例，传统的计算机视觉方法需首先基于经验手动设计特征，然后使用分类器分类，这两个过程都是分开的。而深度学习里的卷积网络可实现对局部区域信息的提取，获得更高级的特征，当神经网络层数越多时，提取的特征会更抽象，将更有助于分类，同时神经网路将提取特征和分类融合在一个结构中。

#### **25. 常用的池化操作有哪些？**

1.Max pooling:选取滑动窗口的最大值

2.Average pooling：平均滑动串口的所有值

3.Global average pooling：平均每页特征图的所有值

优点：

1.解决全连接层所造成的过拟合问题

CNN网络需要将特征图reshape成全连接层，然后再连接输出层，而global average pooling不需要此操作，直接将特征图pooling成输出层

2.没有权重参数

#### **26. 1*1卷积核的作用？**

1.跨通道信息的融合；

2.通过对通道数的降维和升维，减少计算量；

#### **27.为什么ReLU常用于神经网络的激活函数？**

1.在前向传播和反向传播过程中，ReLU相比于Sigmoid等激活函数计算量小；

2.在反向传播过程中，Sigmoid函数存在饱和区，若激活值进入饱和区，则其梯度更新值非常小，导致出现梯度消失的现象。而ReLU没有饱和区，可避免此问题；

3.ReLU可令部分神经元输出为0，造成网络的稀疏性，减少前后层参数对当前层参数的影响，提升了模型的泛化性能；

**28.卷积层和全连接层的区别？**

1.卷积层是局部连接，所以提取的是局部信息；全连接层是全局连接，所以提取的是全局信息；
 2.当卷积层的局部连接是全局连接时，全连接层是卷积层的特例；

#### **29.机器学习和深度学习的区别？**

机器学习在训练模型之前，需要手动设置特征，即需要做特征工程；深度学习可自动提取特征；所以深度学习自动提取的特征比机器学习手动设置的特征鲁棒性更好；

#### **30. 神经网络的优缺点？**

优点:

1.拟合复杂的函数

随着神经网络层数的加深，网络的非线性程度越来越高，从而可拟合更加复杂的函数；

2.结构灵活

神经网络的结构可根据具体的任务进行相应的调整，选择适合的网络结构；

3.神经网络可自动提取特征，比人工设置的特征鲁棒性更好；

缺点：

1.由于神经网络强大的假设空间，使得神经网络极易陷入局部最优，使得模型的泛化能力较差；

2.当网络层数深时，神经网络在训练过程中容易产生梯度消失和梯度下降的问题；

3.随着网络层数的加深，神经网络收敛速度越来越慢；

4.神经网络训练参数多，占用内存大；


#### **31.解决过拟合的方法**

1.数据增强
2.Ealy stopping
3.Dropout
4.交叉验证
5.L1、L2正则化



### **（二）图像方面**


1.opencv遍历像素的方式

[opencv遍历像素的方式](https://blog.csdn.net/qq_34555202/article/details/82020939)
