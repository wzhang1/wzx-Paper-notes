[CVPR2018 语义分割](https://blog.csdn.net/qq_27875705/article/details/80880429)


FEELVOS: Fast End-to-End Embedding Learning for Video Object Segmentation
-----------------

作者：Paul Voigtlaender, Yuning Chai, Florian Schroff, Hartwig Adam, Bastian Leibe, Liang-Chieh Chen

论文链接：https://arxiv.org/abs/1902.09513

摘要: Many of the recent successful methods for video object segmentation (VOS) are overly complicated, heavily rely on fine-tuning on the first frame, and/or are slow, and are hence of limited practical use. In this work, we propose FEELVOS as a simple and fast method which does not rely on fine-tuning. In order to segment a video, for each frame FEELVOS uses a semantic pixel-wise embedding together with a global and a local matching mechanism to transfer information from the first frame and from the previous frame of the video to the current frame. In contrast to previous work, our embedding is only used as an internal guidance of a convolutional network. Our novel dynamic segmentation head allows us to train the network, including the embedding, end-to-end for the multiple object segmentation task with a cross entropy loss. We achieve a new state of the art in video object segmentation without fine-tuning on the DAVIS 2017 validation set with a J&F measure of 69.1%.
最近许多成功的视频对象分割（VOS）方法过于复杂，严重依赖于第一帧的微调和/或速度慢，因此实际应用有限。在这项工作中，我们提出了一种简单而快速的方法，不依赖于微调。为了分割视频，Feelvos对每一帧使用语义像素嵌入和全局和局部匹配机制，将信息从第一帧和视频的前一帧传输到当前帧。与之前的工作相比，我们的嵌入仅用作卷积网络的内部指导。我们的动态分割头使我们能够训练网络，包括嵌入，端到端的多目标分割任务与交叉熵损失。我们在视频对象分割方面取得了新的进展，而没有对Davis 2017验证集进行微调，j&amp;f测量值为69.1%。


论文题目：FickleNet: Weakly and Semi-supervised Semantic Image Segmentation using Stochastic Inference
--------------------------------------------

作者：Jungbeom Lee, Eunji Kim, Sungmin Lee, Jangho Lee, Sungroh Yoon

论文链接：https://arxiv.org/abs/1902.10421

摘要: The main obstacle to weakly supervised semantic image segmentation is the difficulty of obtaining pixel-level information from coarse image-level annotations. Most methods based on image-level annotations use localization maps obtained from the classifier, but these only focus on the small discriminative parts of objects and do not capture precise boundaries. FickleNet explores diverse combinations of locations on feature maps created by generic deep neural networks. It selects hidden units randomly and then uses them to obtain activation scores for image classification. FickleNet implicitly learns the coherence of each location in the feature maps, resulting in a localization map which identifies both discriminative and other parts of objects. The ensemble effects are obtained from a single network by selecting random hidden unit pairs, which means that a variety of localization maps are generated from a single image. Our approach does not require any additional training steps and only adds a simple layer to a standard convolutional neural network; nevertheless it outperforms recent comparable techniques on the Pascal VOC 2012 benchmark in both weakly and semi-supervised settings.

[Decoders对于语义分割的重要性 | CVPR 2019](https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/88967613)
----------------------------------

《An End-to-end Network for Panoptic Segmentation》：在全景分割研究领域中，旷视提出了一种新颖的端到端的全景分割模型。
----------------------------------------------------------------

论文摘要：

全景分割，是需要为图像中每个像素分配类别标签的同时，分割每个目标实例的一种分割任务。这是一个具有挑战性的研究领域，传统的方法使用两个独立的模型但二者之间不共享目标特征，这将导致模型实现的效率很低。此外，传统方法通过一种启发式方法来合成两种模型的结果，在合并过程期间无法利用足够的特征上下文信息，这就导致模型难以确定每个目标实例之间的重叠关系。为了解决这些问题，本文提出了一种新颖的端到端全景分割模型，能够有效地、高效地预测单个网络中每个目标实例及其分割结果。此外，还引入了一种新颖的空间排序模块来处理所预测的实例之间的重叠关系问题。大量的实验结果表明，所提出的方法能够在 COCO Panoptic 基准上取得了非常有前景的结果。 


《DFANet: Deep Feature Aggregation for Real-Time Semantic Segmentation》：今年唉语义分割研究方向，旷视提出一种非常有效的 CNN 架构——DFANet，一种在有限资源下，用于实时语义分割的深度特征聚合算法。
----------------------------


论文摘要：

本文提出一种在有限资源条件下的语义分割模型 DFANet，这是一种非常有效的 CNN 架构。DFANet 从单个轻量级骨干网络开始，分别通过子网和子级联的方式聚合判别性特征。基于多尺度特征的传播，DFANet 网络在获得足够感受野的同时下，大大减少了模型的参数量，提高了模型的学习能力，并在分割速度和分割性能之间取得了很好的平衡。通过在 Cityscapes 和 CamVid 数据集上的大量实验评估，验证了 DFANet 网络的优越性能：相比于最先进的实时语义分割方法，DFANet 网络的分割速度快了3倍，而只使用七分之一的 FLOP，同时保持相当的分割准确性。具体来说，在一块NVIDIA Titan X卡上，对于1024×1024输入，DFANet 在 Cityscapes 测试数据集上实现了71％的平均 IOU (Mean IOU)，分割速度为170FPS，而仅有3.4 GFLOP。同时，当使用较小的骨干模型时，它能够取得67％的平均IOU (Mean IOU)，250 FPS 的分割速度和2.1 GFLOP。


UNet++: A Nested U-Net Architecture for Medical Image Segmentation

将不同尺度UNet拼接

[UNet++: A Nested U-Net Architecture for Medical Image Segmentation](https://arxiv.org/pdf/1807.10165.pdf)


[CGNet: A Light-weight Context Guided Network for Semantic Segmentation](https://www.jianshu.com/p/0e4890e3bae8)

提出了Context Guided (CG) block，可学习局部特征和周围环境的联合特征。 参数小于0.5M的分割模型，可运行于移动设备。 单卡可达50 fps。 缺点是在Cityscapes上的IoU仅有64.8％。
