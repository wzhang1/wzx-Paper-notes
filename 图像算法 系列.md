[目标检测]
----

前言
----
深度学习助力目标检测达到了一个前所未有的高峰。

自 2014 年以来，目标检测框架分为two-stage 和 one-stage 两大类，

前者以经典方法 Faster R-CNN 为代表，

后者以 YOLO 和 SSD 为主要框架。
近年来，两类最深刻的idea：更好的基础网络 & 融合不同卷积层的特征叠加在经典方法上，产生了大量变体



目标检测三个指标：

1.IOU:也即判别预测的方框和真实的方框有多接近的一个指标。

2.mAp:在多个类别的检测中，每一个类别都可以调整阈值，算出召回率（召回率又称查全率）从0到1时的准确率（同一召回率取最高的准确率），计算准确率的平均值，而后再对于所有类求平均得到 mAP。这个值介于0到1之间，且越大越好。简单来讲就是：mAP就是准确率宇查全率的一个参数，就是检测水平

3.fps：速率

目标检测算法的目标即为：定位更准、速度更快、分类更精确。

整个的目标检测框架也分为两大类：

第一是 two-stage 两步走的框架，先进行区域推荐，再进行目标分类；

另一个是 one-stage 端到端的框架，应用一个网络把所有事情都做了，一步输出结果。


![](https://pic4.zhimg.com/80/v2-68516fd46100fa12e9bf28dafa317c2f_hd.jpg)

![](https://pic2.zhimg.com/80/v2-d4899d7ca6cfb442a4e2f1d276d2defd_hd.jpg)

![](https://pic4.zhimg.com/80/v2-081bcb21d00f06874ca0bf75781b7daf_hd.jpg)



[RCNN（CVPR，2014）](https://cjmcv.github.io/deeplearning-paper-notes/fdetect/2016/01/02/RCNN.html)
----

需补充知识（Alexnet；VGG16）

作者：RBG（Ross B. Girshick）大神，不仅学术牛，工程也牛，代码健壮，文档详细，clone下来就能跑。
RCNN脉络

RCNN--SPPNET--Fast-RCNN--Faster-RCNN


![](https://pic2.zhimg.com/v2-0c98fb30a9e589fa164d99c50e6ca711_r.jpg)

参考资料:

[图解CNN论文：尝试用最少的数学读懂深度学习论文](https://www.bilibili.com/video/av22822657/)

[RCNN- 将CNN引入目标检测的开山之作](https://zhuanlan.zhihu.com/p/23006190)

RCNN (论文：Rich feature hierarchies for accurate object detection and semantic segmentation) 是将CNN方法引入目标检测领域， 大大提高了目标检测效果，可以说改变了目标检测领域的主要研究思路， 紧随其后的系列文章：（ RCNN）,Fast RCNN, Faster RCNN 代表该领域当前最高水准。

作者：周新一
链接：https://www.zhihu.com/question/35887527/answer/77490432
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

RCNN的一个出发点在文章第一句话：features matter。传统的物体检测使用hand-engineered的特征。如HOG特征可以表示为filter(convolutional)+gating(nonlinear)+pooling+normalization(LRN)，很多传统特征可以用图片经过CNN后得到的feature map表示。并且，由于卷积操作具有平移不变形，feature map里不仅包含了物体的what信息，还包含着物体的where信息。因此可以利用CNN特征来代替传统特征，对特征用SVM得到分类结果，同时可以对特征作回归得到更精确的位置。

SPP首先解决传统的CNN输入需要图片长宽固定的问题（这个问题在基于区域的输入上更为明显），把原来的“从原图上截取区域”转化为“从feature map上截取区域”，有了这个操作，同一张图片上不同区域的物体检测可以共享同一个原图特征。然而SPP的区域特征提取是在卷积层参数固定后才进行的，并且要额外进行SVM分类和区域框回归。

Fast RCNN解决了两个技术问题：1）如何让区域的选择可导（提出ROI Pooling操作，区域特征选择像Max Pooling一样可导），2）如何让SGD高效（把同一张图的不同区域放在同一个batch里），并且把原来的区域框回归作为一个Multitask接在网络里（Smooth L1 norm），这样除了区域推荐外的全部任务可以在一个网络完成。Faster RCNN把区域推荐也放在网络里完成（RPN）。这样整个框架都可以在一个网络里运行。

然而，Faster RCNN的训练为了保证卷积特征一致性需要分4步训练。RBG在Tutorial上表示已有让区域推荐可导（类似Spatial Transform Network中的sampler操作）的joint training。

论文算法概述

   整个算法过程包含三个步骤：a、输入图像，搜索生成类别独立的物体候选框； b、使用大的CNN网络对各候选框提取固定长度的特征向量；c、利用线性SVM对特征向量进行分类识别。
   
RCNN算法分为4个步骤 

候选区域生成： 一张图像生成1K~2K个候选区域 （采用Selective Search 方法）

特征提取： 对每个候选区域，使用深度卷积网络提取特征 （CNN） 

类别判断： 特征送入每一类的SVM 分类器，判别是否属于该类 

位置精修： 使用回归器精细修正候选框位置 (重叠度（IOU）+非极大值抑制（NMS）)

   
  ![](https://cjmcv.github.io/deeplearning-paper-notes/images/pdDetect/rcnn1.jpg)
  
存在问题
  
训练过程繁琐：包含基于物体候选框使用log损失微调卷积网络；基于调优后的卷积特征训练SVM；训练bounding box回归器（参考DPM）；
训练耗时，占用磁盘空间大。
每个候选框需要进行整个前向CNN计算导致测试速度慢：

总结

算法分三步，1）selective；2）将各候选框缩放填充至固定大小，用一个CNN对分别提取特定长度的特征向量；3）SVM对向量分类。主要短板在第2步，每个框都要重复输入CNN进行独立的计算。
  
  [SPP-Net（ECCV，2014）](https://cjmcv.github.io/deeplearning-paper-notes/fdetect/2016/02/05/SPP.html)
  ----
  
  
  论文算法概述
  
   SPP（spatial pyramid pooling，空间金字塔池化）：1）SPP层可以不论输入大小都能产生固定长度的输出，而传统基于滑动窗口的池化层则不行；2）SPP层使用多空间尺度的bins，而基于滑动窗口的池化层则使用单一尺寸窗口。多尺度的池化对于物体变型更具有鲁棒性；   3）由于可以输入任意尺度，SPP可以提取多尺度的池化特征；
  
 1、 R-CNN中输入的候选窗口需要归一化到一定大小，如图所示裁剪和变换都可能会造成信息丢失导致识别率下降。而SPP-net可以解决这个问题。

2、R-CNN在检测图像时使用selective search提取出很多个候选框（2000个），然后每个候选框都各自进行一次前向CNN特征提取与线性SVM分类，因此很耗时。而SPP-net则避免了重复计算卷积特征。

何凯明团队的SPPNet给出的解决方案是，既然只有全连接层需要固定的输入，那么我们在全连接层前加入一个网络层，让他对任意的输入产生固定的输出不就好了吗？一种常见的想法是对于最后一层卷积层的输出pooling一下，但是这个pooling窗口的尺寸及步伐设置为相对值，也就是输出尺寸的一个比例值，这样对于任意输入经过这层后都能得到一个固定的输出。SPPnet在这个想法上继续加入SPM的思路，SPM其实在传统的机器学习特征提取中很常用，主要思路就是对于一副图像分成若干尺度的一些块，比如一幅图像分成1份，4份，8份等。然后对于每一块提取特征然后融合在一起，这样就可以兼容多个尺度的特征啦。SPPNet首次将这种思想应用在CNN中，对于卷积层特征我们也先给他分成不同的尺寸，然后每个尺寸提取一个固定维度的特征，最后拼接这些特征不就是一个固定维度的输入了吗？
![](https://cjmcv.github.io/deeplearning-paper-notes/images/pdDetect/spp1.jpg)
![](https://cjmcv.github.io/deeplearning-paper-notes/images/pdDetect/spp2.jpg)

存在问题

1、和RCNN一样，训练过程繁琐，包含基于物体候选框使用log损失微调卷积网络；基于调优后的卷积特征训练SVM；训练bounding box回归器（参考DPM）；

2、和RCNN一样，训练结果占用空间很大，需要保存在磁盘中；

3、与RCNN不同，SPP-Net无法更新SPP层前面的卷积层，因为CNN调优和SVM训练分离，SVM的loss无法传播到卷积层（RCNN也一样）,即意味着只能微调全连接层，而对于很多情况下卷积层也是需要微调的，特别是层次更深的网络。

实验结果

其中单一尺寸训练结果低于RCNN1.2%，但是速度是其102倍，5个尺寸的训练结果与RCNN相当，其速度为RCNN的38倍。

总结

SPP叫空间金字塔池化，主要是为了处理RCNN中需要将候选框的大小归一化造成的损失，以及每个框单独重复计算的问题。
这里前面的操作与RCNN一致，不同点是对输入全图图像做一次CNN前向，然后在根据SS的候选框去切分特征图，得到一个个大小不定的特征图块，将每个特征图块输入到一个SPP层上得到一个固定长度的特征向量，再进入fc层。

即把候选框切分图像块改成切分特征图块，用SPP层解决固定长度的问题。

PS:

空间金字塔池化，设金字塔有三层，分别对应1x1、2x2和4x4的池化，设输入特征图通道数为256，则1x1池化得到1x1x256d向量，2x2池化得到2x2x256d，4x4池化得到4x4x256d，则最终的特征向量长度为三个数累加，维度固定，与输入特征图大小无关。

[Fast-RCNN（ICCV，2015）](https://cjmcv.github.io/deeplearning-paper-notes/fdetect/2016/01/04/FRCNN.html)
----
参考资料

[Fast Rcnn git hub 代码](https://github.com/yhenon/keras-frcnn)

[图解Fast RCNN](https://www.bilibili.com/video/av22822657/?p=44)

回顾RCNN

在RCNN中，可以看到还是存在很多问题。其中最主要的就是训练过程复杂：

先预训练

再利用selective search产生的region proposals进行微调CNN网络

再用CNN提取到的region proposal的固定长度的特征向量来训练一组SVM

最后还要训练一组BBox回归器，还是需要用CNN来提取特征

其中2，3，4中样本数据的构造是不一样的，所以还要分别构造各自的数据集。训练过程中反复用到CNN（要么就用额外的存储来保存特征向量，代价很大）。

除了训练，测试过程也比较慢，因为需要将产生的2k个region proposals依次通过CNN来提取特征（没有共享计算导致的结果）。

当然这些都不能否定RCNN在图像检测方面的突破。下面开始进入Fast R-CNN.

改进

很大程度上实现了end to end（除了region proposals的产生还是用的selective search）。

不再是将region proposals依次通过CNN，而是直接输入原图，来提取特征（这样一张图只会CNN一次）。

网络同时输出类别判断以及bbox回归建议（即两个同级输出），不再分开训练SVM和回归器。

![网络结构](https://pic4.zhimg.com/80/v2-d825cbb7a7ab0d15c559c2d595b6a4a2_hd.jpg)

![具体网络结构](https://pic3.zhimg.com/80/v2-1ef48d2473e783a4bdc7d66ee5f8a083_hd.jpg)

[FASTER RCNN](https://cjmcv.github.io/deeplearning-paper-notes/fdetect/2016/01/06/FRCNN2.html)
----
参考资料

[keras版faster-rcnn算法详解](https://zhuanlan.zhihu.com/p/28585873)

[faster -rcnn 代码详解](https://zhuanlan.zhihu.com/p/31530023)

[目标检测——从RCNN到Faster RCNN 串烧](https://blog.csdn.net/xyy19920105/article/details/50817725)

经过R-CNN和Fast RCNN的积淀，Ross B. Girshick在2016年提出了新的Faster R-CNN，在结构上，Faster RCNN已经将特征抽取(feature extraction)，proposal提取，bounding box regression(rect refine)，classification都整合在了一个网络中，使得综合性能有较大提高，在检测速度方面尤为明显。

![](https://pic1.zhimg.com/80/v2-e64a99b38f411c337f538eb5f093bdf3_hd.jpg)

![Faster RCNN训练步骤](https://pic1.zhimg.com/80/v2-ed3148b3b8bc3fbfc433c7af31fe67d5_hd.jpg)

总结

主要是提出了RPN，代替了Fast-RCNN上的使用的selective-search，其余结构未变。而RPN中设定3个尺度和3个形状组成9个anchors，以滑动窗口的方式搜索目标。其中滑动窗口以n x n卷积实现，卷积输出多个特征图，即使每个滑动窗口得到一个多维向量。然后再用1x1x18和1x1x36卷积去扫描，分别得到对应每个位置上9个anchor的类别和边框的预测。

[A-Fast-RCNN（CVPR, 2017）](https://cjmcv.github.io/deeplearning-paper-notes/fdetect/fgan/2017/04/30/AFastRCNN.html)
----

Motivation

这篇文章提出了一种新的对手生成策略，通过训练提升检测网络对遮挡、形变物体的识别精度。

遮挡和形变是检测任务中影响模型性能的两个显著因素。增加网络对遮挡和形变的鲁棒性的一个方式是增加数据库的体量。但是由于遮挡的图片一般都处在图像分布的长尾部分，即便增加数据，遮挡和形变的图片仍是占比较少的部分。另一个思路就是使用生成网络产生遮挡和形变图片。然而遮挡和形变的情况太多，直接生成这些图片还是比较困难的事情。在这篇文章中，作者的思路是训练一个对手网络，让这个网络动态产生困难的遮挡和形变样本。以遮挡为例，作者希望被训练的检测网络会认为哪里的遮挡更难以处理。之后去遮挡这些区域的特征，从而让检测网络努力学习这些困难的遮挡情况。对于形变的情况同理。与其它提升检测网络性能的方法，如换用更强大的主干网络、使用自上而下的网络设计等相比，本文提出的方法则是考虑如何更加充分地利用训练数据。

[作者：kfxw
链接：https://zhuanlan.zhihu.com/p/33936283
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。]

![](https://cjmcv.github.io/deeplearning-paper-notes/images/pdDetect/afastrcnn1.png)

Experiments

在VOC2007上的结果如下图所示，与Online Hard Example Mining（OHEM）相比，在VOC2007上文中结果（71.4%）比OHEM（69.9%）要好，但在VOC2012上文中结果（69.0%）比OHEM（69.8%）要稍低一点。作者认为这两种方法可以互补，实验中将两种方法联合，在VOC2012上可以达到71.7%；而两个OHEM模型联合为71.2%，两个文中的模型联合则为70.2%。


[MASK RCNN](https://zhuanlan.zhihu.com/p/25954683)

Mask R-CNN是一个小巧、灵活的通用对象实例分割框架（object instance segmentation）。

它不仅可对图像中的目标进行检测，还可以对每一个目标给出一个高质量的分割结果。

它在Faster R-CNN[1]基础之上进行扩展，并行地在bounding box recognition分支上添加一个用于预测目标掩模（object mask）的新分支。

该网络还很容易扩展到其他任务中，比如估计人的姿势，也就是关键点识别（person keypoint detection）。该框架在COCO的一些列挑战任务重都取得了最好的结果，包括实例分割（instance segmentation）、候选框目标检测（bounding-box object detection）和人关键点检测（person keypoint detection）。

![](https://pic1.zhimg.com/80/v2-7f92e9a39cae4786fbb4cd3d573eb972_hd.jpg)

一共可以分为两个分支：

（1）第一个分支为原始Faster R-CNN的结构，它用于对候选窗口进行分类和窗口坐标回归。

（2）第二个分支对每一个感兴趣区域（Region of Interest，RoI）预测分割掩模，它利用了一个小的全卷积网络结构[2]（Fully Convolutional Network，FCN）。

3 主要关键因素

关键点1：解决特征图与原始图像上的RoI不对准问题（没懂）

关键点2：将掩模预测和分类预测拆解

简单来讲就是先用fast-rcnn找到感兴趣区域，然后用fcn预测该区域的掩模

关键点3：掩模表示（没懂）

一个掩模编码了一个输入对象的空间布局。作者使用了一个FCN来对每个RoI预测一个m\times m的掩模，这保留了空间结构信息。







YOLO
----


[SCNN](https://blog.csdn.net/u011974639/article/details/79580798)
----

语义分割--(SCNN)Spatial As Deep: Spatial CNN for Traffic Scene Understanding

简述

论文提出了一个新颖网络Spatial CNN，在图片的行和列上做信息传递。

可以有效的识别强先验结构的目标。(道路，墙，路灯等)

论文提出了一个大型的车道检测数据集，用于进一步推动自动驾驶发展。

[代码](https://github.com/XingangPan/SCNN)

abstract

CNN架构没有足够充分探索图像行和列上的空间关系能力。这些关系对于学习强先验形状的对象很重要，尤其是外观(图像像素)连贯性很弱。例如交通线，车道经常会被遮挡，或者压根就没在路上画车道线。如下图所示：

![](http://owv7la1di.bkt.clouddn.com/blog/180316/F8Ae40dj22.png?imageslim)

本文提出了Spatial CNN(CNN),它将传统的卷积层接层(layer-by-layer)的连接形式的转为feature map中片连片卷积(slice-by-slice)的形式，使得图中像素行和列之间能够传递信息。这特别适用于检测长距离连续形状的目标或大型目标，有着极强的空间关系但是外观线索较差的目标，例如交通线，电线杆和墙。论文在车道检测挑战和CityScapes上评估了SCNN的表现，同时SCNN在TuSimple Benchmark lane Detection challenge获得了第一名，准确率为96.53%。

网络结构：

![](http://owv7la1di.bkt.clouddn.com/blog/180316/fc1jfk8j39.png?imageslim)

(图中SCNN的下标有D,U,R,L，这在结构上是类似的，方向上分别表示为向下，向上，向右，向左)

Conclusion

论文提出了Spatial CNN，在空间层上实现信息的有效传递。SCNN易于融入到其他深度神经网络中做end-2-end训练。论文在车道检测和语义分割上测试了SCNN，结果表现SCNN可以有效的保持长距离连续结构，在语义分割其扩散效应对识别大型物体有利。

此外，论文提出了一个车道检测的数据集，希望能够推动自动驾驶进一步发展。

[RON](https://blog.csdn.net/zhangjunhit/article/details/77717569)
-----
本文可以看作是对 SSD 的改进， SSD 对不同尺度特征图进行独立的检测，这里我们 reverse connection block 将相邻的特征图联系起来。同时使用 objectness prior 来有效降低目标的搜索空间。

![](https://img-blog.csdn.net/20170830153505641?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvemhhbmdqdW5oaXQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)


[SSD](https://cjmcv.github.io/deeplearning-paper-notes/fdetect/2016/02/12/SSD.html)
----

Single Shot MultiBox Detector

简述：

在VGG16基础上，将最后将全连接层变成卷积层

单一的深度神经网络用于检测多个类别的物体，可进行端到端的训练，速度比之前的single shot detectors（YOLO）快，准确率与Faster-RCNN相当，甚至对于低分辨率的图像也能取得不差的效果。而其核心在于使用应用于特征图上的小卷积滤波器进行物体类别与区域预测，如图1某层8 x 8大小的特征图，使用3 x 3的滑窗提取每个位置的特征，然后这个特征回归得到目标的坐标信息和类别信息。为了实现高检测率，在不同尺度的特征图上进行多尺度的预测，并根据长宽比来分离预测结果。

![](https://cjmcv.github.io/deeplearning-paper-notes/images/pdDetect/ssd1.png)

用特征图金子塔进行多尺度检测，在骨干网络后接几个卷积层，使输出特征图的尺度一层比一层小，形成多尺度的金字塔。金字塔上每个特征图都单独预测。最后将所有层的预测合并在一起做NMS。

其中还提了一个先验框的概念，在金子塔上每层分别使用的多个先验框（数量不一定相等），具体方式与anchor相似，以滑动窗口扫描（conv3x3 pad1，则输出特征图与输入尺度不变），每点上会对应着在该点上各个先验框的类别信息和位置信息，存储在个通道上。如某层采用6个先验框，则对分类输出的特征图的通道数应为6x(20+1)=126，20是类别1为背景，即输入特征图每个点会得到1x1x126的特征向量来表示其分类信息。同理对定位输出的则为6x4=24。

[Lighten CNN](https://cjmcv.github.io/deeplearning-paper-notes/freg/2015/12/16/LightenCNN.html)
----

提出一个轻量级的CNN网络结构，可在包含大量噪声的训练样本中训练人脸识别任务。

在CNN的每层卷积层中引入了maxout激活概念，得到一个具有少量参数的Max-Feature-Map(MFM)。与ReLU通过阈值或偏置来抑制神经元的做法不同，MFM是通过竞争关系来抑制。不仅可以将噪声信号和有用信号分离开来，还可以在特征选择上起来关键作用。

该网络基于MFM，有5个卷积层和4个Network in Network（NIN）层。小的卷积核与NIN是为了减少参数，提升性能。

采用通过预训练模型的一种semantic bootstrapping的方法，提高模型在噪声样本中的稳定性。错误样本可以通过预测的概率被检测出来。

实验证明该网络可以在包含大量噪声的训练样本中训练轻量级的模型，而单模型输出256维特征向量，在5个人脸测试集上达到state-of-art的效果。且在CPU上速度达到67ms。

![](https://cjmcv.github.io/deeplearning-paper-notes/images/pdReg/lightencnn1.png)


[FCN](https://zhuanlan.zhihu.com/p/22976342)
----

全卷积网络首现于这篇文章。这篇文章是将CNN结构应用到图像语义分割领域并取得突出结果的开山之作，因而拿到了CVPR 2015年的best paper honorable mention.

图像语义分割，简而言之就是对一张图片上的所有像素点进行分类

![](https://pic2.zhimg.com/80/v2-aec05f2f4b85238dc74724aeedbfc79b_hd.jpg)

下面我们重点看一下FCN所用到的三种技术:

1.卷积化(convolutionalization)



分类所使用的网络通常会在最后连接全连接层，它会将原来二维的矩阵(图片)压缩成一维的，从而丢失了空间信息，最后训练输出一个标量，这就是我们的分类标签。

而图像语义分割的输出则需要是个分割图，且不论尺寸大小，但是至少是二维的。所以，我们丢弃全连接层，换上卷积层，而这就是所谓的卷积化了。
![](https://pic3.zhimg.com/80/v2-42d85c5f7ddcb3f527666b250f62f5d6_hd.jpg)

这幅图显示了卷积化的过程,图中显示的是AlexNet的结构，简单来说卷积化就是将其最后三层全连接层全部替换成卷积层

三.总结

图像语义分割可能是自动驾驶技术下一步发展的一个重要突破点，




[UDC_HDC(dense upsampling convolution_hybrid dilated convolution密集的上采样卷积_混合扩张卷积)](https://cjmcv.github.io/deeplearning-paper-notes/fmask/2017/05/10/UDC_HDC.html)
----
[DUC代码](https://link.zhihu.com/?target=https%3A//github.com/ycszen/pytorch-ss)

[【简评】Understanding Convolution for Semantic Segmentation](https://zhuanlan.zhihu.com/p/26659914)


基础知识：

up sampling 上采样: 放大图像

原理：图像放大几乎都是采用内插值方法，即在原有图像像素的基础上在像素点之间采用合适的插值算法插入新的元素



subsampled 下采样：缩小图像

原理：若矩阵形式的图像，就是把原始图像s*s窗口内的图像变成一个像素，这个像素点的值就是窗口内所有像素的均值（简单来讲，很像池化poolin层）

卷积：

[卷积计算动画](https://github.com/vdumoulin/conv_arithmetic)

![](https://images2017.cnblogs.com/blog/1300850/201712/1300850-20171217153715843-645064706.png)

![](https://images2017.cnblogs.com/blog/1300850/201712/1300850-20171217154759936-1845443467.png)

逆卷积（转置卷积）：

逆卷积相对于卷积在神经网络结构的正向和反向传播中做相反的运算

应用：GAN中生成图片

转置卷积只能还原shape大小，不能还原value

![](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/padding_strides_odd_transposed.gif)

![](https://pic1.zhimg.com/80/v2-a08093f7d93d6e395205b7e22bcf41ec_hd.jpg)

Dilated convolution animations(扩张卷积)：

N.B.: Blue maps are inputs, and cyan maps are outputs.

![](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/dilation.gif)


DUC:

dense updampling convolution——通过学习一些系列的放大的过滤器来放大降采样的feature map到最终想要的尺寸。具体结构如下：

![](https://pic1.zhimg.com/80/v2-c98e6474dbb566d3ca62c65a73ee4fa0_hd.jpg)

设输入图像高宽为H/W，通道数为C，则像素级的语义分割就是生成H x W的标签图，每个像素都有一个类别信息。在输入到一个深度全卷积网络后，在进行预测之前会得到一个h x w x c的特征图，其中h=H/r，w=W/r，r为下采样因子。DUC使用卷积操作直接在特征图上得到逐像素的预测结果，而不像双线性上采样那样参数不可训练，也不像反卷积那样在卷积操作之前的非池化步骤中需要使用0去填充。

DUC是可训练的，所以它有能力去提取和恢复在双线性插值上很容易丢失的细粒度信息。

<font color=red>也许可用的创新点:将DUC应用在GAN中的generate内容</font>
----


HDC

HDC主要是为了解决使用dilated convolution会产生的“gridding issue”。

HDC主要是为了解决使用扩散卷积会产生的“网格问题”。

第一行是ground truth，第二行是gridding现象，第三行是使用HDC后的输出

![](https://pic4.zhimg.com/80/v2-d5e35e52da13add7b80c1cd529719de9_hd.jpg)

[DEEP FACE](https://zhuanlan.zhihu.com/p/36630496)
----

传统的人脸识别流程是检测--对齐--特征表示--分类。

这里主要考虑对齐和特征表示步骤，采用显式的3D人脸模型做变换对齐，使用一个9层的网络模型做特征表示

![](https://cjmcv.github.io/deeplearning-paper-notes/images/pdReg/deepface2.png)

![](https://pic1.zhimg.com/80/v2-e733cea8f90053137ed56cf1c662ae22_hd.jpg)


[PCN 一种基于渐进式校准神经网络的实时旋转不变性人脸检测方法]()
----


[code ](github.com/Jack-CV/PCN)

![](https://pic2.zhimg.com/80/v2-2853cbc19c88b9e3b9a74ef6cf11ee7d_hd.jpg)

PCN包含三个阶段:
1.输入原始图片，如果图片中的人脸角度在-180度到-90度之间或90度到180度之间，则将人脸旋转180度，使得最终输出的所有人脸角度均在-90度到90度之间；

2.第二阶段的输入为第一阶段校正后的人脸图片，继续缩小人脸角度，将人脸角度限制到-45-45度；

3第三阶段将角度最终校准到0度。为了避免检测过程中的旋转操作，本文直接在图片输入网络之前做了旋转，得到四种类型的图像。

![](https://pic4.zhimg.com/80/v2-8db1aed06cf7cea7c238e9f18dbf4bf7_hd.jpg)

总结：
1 旋转人脸问题如果一个模型all in，耗时会很大(如用frcnn、ssd去cover)；

2 作者提出了pcn，采用mtcnn、cascade cnn的操作流程，逐步渐进地调整旋转人脸的角度，使得最终的人脸角度调整至[-45. 45]之间，再使用普通的人脸检测器即可检测出人脸；

3 渐进调整在stage1、2十分快速、操作简单、预测也准确(分类问题)，耗时少；

4 stage3再在调整好之后的candidates上进一步检测人脸、调整bbox，整体上效果就非常之好了；
