神经网络加速
====

参考资料

[当前深度神经网络模型压缩和加速都有哪些方法](https://zhuanlan.zhihu.com/p/36051603)


过去我们的深度神经网络大部分都是在GPU上或高性能的CPU上做计算，对加速压缩的需求没有那么强烈。但当深度神经网络在应用到具体场景时，有很多情况下必须在智能手机、穿戴设备等嵌入式设备上使用。这些嵌入式设备对模型体积、计算性能、功耗等方面都有比较严格的要求，这就限制了上述对计算性能要求较高的深度神经网络模型的应用。

![](https://pic3.zhimg.com/80/v2-1ed310983131c8480d4427025c1ea97a_hd.jpg)


研究现状
综合现有的深度模型压缩方法，它们主要分为四类：

参数修剪和共享（parameter pruning and sharing）：基于参数修剪和共享的方法针对模型参数的冗余性，试图去除冗余和不重要的项。

低秩因子分解（low-rank factorization）：基于低秩因子分解的技术使用矩阵/张量分解来估计深度学习模型的信息参数。

转移/紧凑卷积滤波器（transferred/compact convolutional filters）：。基于传输/紧凑卷积滤波器的方法设计了特殊的结构卷积滤波器来降低存储和计算复杂度。

知识蒸馏（knowledge distillation）：知识蒸馏方法通过学习一个蒸馏模型，训练一个更紧凑的神经网络来重现一个更大的网络的输出。

![](https://pic2.zhimg.com/80/v2-40d105acac9439f75fb60702f591bd5b_hd.jpg)

参数修剪和共享
----

根据减少冗余（信息冗余或参数空间冗余）的方式，这些参数修剪和共享可以进一步分为三类：


模型量化和二进制化:


![]（https://pic2.zhimg.com/80/v2-9a378037c4026a7a4d81f8254e206eda_hd.jpg）

一套完整的深度网络的压缩流程：首先修剪不重要的连接，重新训练稀疏连接的网络。然后使用权重共享量化连接的权重，再对量化后的权重和码本进行霍夫曼编码，以进一步降低压缩率。如图 2 所示，包含了三阶段的压缩方法：修剪、量化（quantization）和霍夫曼编码。

修剪减少了需要编码的权重数量，量化和霍夫曼编码减少了用于对每个权重编码的比特数。对于大部分元素为 0 的矩阵可以使用稀疏表示，进一步降低空间冗余，且这种压缩机制不会带来任何准确率损失。这篇论文获得了 ICLR 2016 的 Best Paper。

缺点：

在量化级较多的情况下准确率能够较好保持，但对于二值量化网络的准确率在处理大型 CNN 网络，如 GoogleNet 时会大大降低。另一个缺陷是现有的二进制化方法都基于简单的矩阵近似，忽视了二进制化对准确率损失的影响。

剪枝和共享:

网络剪枝和共享起初是解决过拟合问题的，现在更多得被用于降低网络复杂度。

早期所应用的剪枝方法称为偏差权重衰减（Biased Weight Decay），其中最优脑损伤（Optimal Brain Damage）和最优脑手术（Optimal Brain Surgeon）方法，是基于损失函数的 Hessian 矩阵来减少连接的数量。他们的研究表明这种剪枝方法的精确度比基于重要性的剪枝方法（比如 Weight Decay 方法）更高。这个方向最近的一个趋势是在预先训练的 CNN 模型中修剪冗余的、非信息量的权重。

在稀疏性限制的情况下培训紧凑的 CNN 也越来越流行，这些稀疏约束通常作为 l_0 或 l_1 范数调节器在优化问题中引入。

剪枝和共享方法存在一些潜在的问题。首先，若使用了 l_0 或 l_1 正则化，则剪枝方法需要更多的迭代次数才能收敛，此外，所有的剪枝方法都需要手动设置层的超参数，在某些应用中会显得很复杂。

![](https://pic2.zhimg.com/80/v2-a142cf6b751e5ad987d0997d27413415_hd.jpg)

设计结构化矩阵

该方法的原理很简单：如果一个 m×n 阶矩阵只需要少于 m×n 个参数来描述，就是一个结构化矩阵（structured matrix）。

通常这样的结构不仅能减少内存消耗，还能通过快速的矩阵-向量乘法和梯度计算显著加快推理和训练的速度。

这种方法的一个潜在的问题是结构约束会导致精确度的损失，因为约束可能会给模型带来偏差。另一方面，如何找到一个合适的结构矩阵是困难的。没有理论的方法来推导出来。因而该方法没有广泛推广。


低秩分解和稀疏性
----

低秩方法很适合模型压缩和加速，但是低秩方法的实现并不容易，因为它涉及计算成本高昂的分解操作。另一个问题是目前的方法都是逐层执行低秩近似，无法执行全局参数压缩，因为不同的层具备不同的信息。最后，分解需要大量的重新训练来达到收敛。

基于此，深度神经网络加速与压缩的研究目的是：在保证现有模型的性能基本不变的前提下，采用一些方法能够有效的大幅减少计算量、降低模型的体积，那就再好不过了。

迁移/压缩卷积滤波器
----

虽然目前缺乏强有力的理论，但大量的实证证据支持平移不变性和卷积权重共享对于良好预测性能的重要性。

[补充知识：什么是权重共享：第一个答案](https://www.zhihu.com/question/47158818)

这种方法仍有一些小问题解决。首先，这些方法擅长处理广泛/平坦的体系结构（如 VGGNet）网络，而不是狭窄的/特殊的（如 GoogleNet，ResidualNet）。其次，转移的假设有时过于强大，不足以指导算法，导致某些数据集的结果不稳定。

知识蒸馏
----

将深度和宽度的网络压缩成较浅的网络

遵循“学生-教师”的范式减少深度网络的训练量，这种“学生-教师”的范式，即通过软化“教师”的输出而惩罚“学生”。为了完成这一点，学生学要训练以预测教师的输出，即真实的分类标签。这种方法十分简单，但它同样在各种图像分类任务中表现出较好的结果。

能令更深的模型变得更加浅而显著地降低计算成本。但是也有一些缺点，例如只能用于具有 Softmax 损失函数分类任务，这阻碍了其应用。另一个缺点是模型的假设有时太严格，其性能有时比不上其它方法。



深度神经网络加速与压缩主要有以下几种方法：Low-Rank、Pruning、Quantization、Knowledge Distillation、Compact Network Design。

如下图所示是最近一年几个的顶级会议收录的加速与压缩方面的论文情况。可以看到Pruning、Quantization是该领域研究的热点。
![](https://pic3.zhimg.com/80/v2-45f95cc463109b2bc0490429523bfb6c_hd.jpg)



