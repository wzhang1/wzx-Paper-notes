各种知识点记录：



### One shot learning:

其实就是相对于zero-short learning, one-short learning还是有一些训练样本，但是非常少，甚至是只有一个训练样本。大家都知道，在图像识别领域，想要识别准确率高，同一类的训练样本需要非常多来训练才会使得模型拟合较好。


Zero-short learning：

就是用来解决识别我们的训练样本中从没就没有出现过的样本。但是，虽然训练样本中从来没有出现过该样本，但是却有类似的样本出现过。比如，你的训练样本中包括了很多的猫，而且对猫的描述有比较详尽的描述，那么对于新出现的一类猫虽然在训练样本中没有出现过，但是你还是能够给识别出来。

![](https://pic2.zhimg.com/v2-c94bc52d768787b5e6b773ffdb92e339_b.gif)

将特征分离，便于预测接近哪个类别

### 逻辑回归：

核心： 将输入变成离散值输出，用来分类

#### 逻辑回归的优缺点

优点：计算代价低，速度快，容易理解和实现。

缺点：容易欠拟合，分类和回归的精度不高。

### 为什么使用空洞卷积：

空洞卷积实际上是一个卷积核中插入0值，以增大实际卷积核大小。对于一个kernel_size为3x3的卷积核，假如其空洞大小dilation为2，不考虑pad和stride，则实际卷积核大小为kernel_size + (kernel_size - 1) * (dilation - 1)。

最大好处：扩大感受野的同时不丢失信息。

缺点：然而由于空洞卷积的数据格式为[N, H, W, C]，每次数组中寻址都和通道相关，每次计算由于读取区域的内存不连续，造成极高的Cache Miss，且破坏了内存对齐，但这对实际运行效率造成了多大影响还需要更多测试。


### L1,L2 正则化：

1、为什么要用正则化：

一句话解答，为了抑制模型高维参数，使高维参数尽可能约等于0，防止模型过拟合。

![](https://images2018.cnblogs.com/blog/1238724/201807/1238724-20180724172501078-518207268.jpg)

L1：最后用绝对值

L2：最后用平方和


### 为什么使用最大池化：

pooling的结果是使得特征减少，参数减少，但pooling的目的并不仅在于此。

pooling目的是为了保持某种不变性（旋转、平移、伸缩等）

最大池化更多的保留纹理信息。

### 样本类别不均衡问题：

背景比要检测的车道线多很多。

解决方法： focal loss

![](https://images2018.cnblogs.com/blog/1055519/201808/1055519-20180818162755861-24998254.png)

### 手推BP公式：

[知乎](https://zhuanlan.zhihu.com/p/33394477)

[例子](https://blog.csdn.net/dare_kz/article/details/77603522)

![](https://github.com/greenfishflying/wzx-Paper-notes/blob/master/image/bp1.jpeg)

![](https://github.com/greenfishflying/wzx-Paper-notes/blob/master/image/bp2.jpeg)


