机器学习基础知识
====

[很好的资料整理]
----
[Deep Learning（深度学习）学习笔记整理系列之（一）](https://blog.csdn.net/zouxy09/article/details/8775360)

[机器学习算法工程师](https://zhuanlan.zhihu.com/JeemyJohn)


[1.正则化](https://www.zhihu.com/question/20924039/answer/240037674)
----

最最最通俗的理解，loss的一部分，让模型无法完美匹配的项。

对参数假设一个先验分布，L1正则就是拉普拉斯分布，L2正则是高斯分布

![](https://pic1.zhimg.com/80/v2-0b85c9b8efdcb17167e52a1fc6a359f8_hd.jpg)

[2.梯度弥散（梯度消失）/梯度爆炸](https://cjmcv.github.io/deeplearning-paper-notes/fbdnn/2015/01/01/DNN_DIFFU.html)

如果每层梯度大于一，那么多层乘下来的梯度就是一个很大的数（梯度爆炸），如果小于一，那么最后就是一个接近零的数（梯度消失）
解决方案最好的当然是对参数进行normalization

在神经网络中，梯度下降算法是使用非常广泛的优化算法。梯度下降算法的变体有好多，比如随机梯度下降（Stochastic gradient descent，SGD）、小批量梯度下降（Mini Batch Gradient Descent）等，但对于梯度下降算法而言，难免需要考虑梯度下降算法中遇到的梯度弥散以及梯度爆炸等问题

梯度爆炸是什么？误差梯度在网络训练时被用来得到网络参数更新的方向和幅度，进而在正确的方向上以合适的幅度更新网络参数。在深层网络或递归神经网络中，误差梯度在更新中累积得到一个非常大的梯度，这样的梯度会大幅度更新网络参数，进而导致网络不稳定。在极端情况下，权重的值变得特别大，以至于结果会溢出（NaN值，无穷与非数值）。当梯度爆炸发生时，网络层之间反复乘以大于1.0的梯度值使得梯度值成倍增长。梯度爆炸会引发哪些问题？在深度多层感知机网络中，梯度爆炸会导致网络不稳定，最好的结果是无法从训练数据中学习，最坏的结果是由于权重值为NaN而无法更新权重。梯度爆炸会使得学习不稳定；


如何解决梯度爆炸问题？解决梯度爆炸问题的方法有很多，本部分将介绍一些有效的实践方法：

1.重新设计网络模型

在深层神经网络中，梯度爆炸问题可以通过将网络模型的层数变少来解决。此外，在训练网络时，使用较小批量也有一些好处。在循环神经网络中，训练时使用较小时间步长更新（也被称作截断反向传播）可能会降低梯度爆炸发生的概率。

2.使用修正线性激活函数

在深度多层感知机中，当激活函数选择为一些之前常用的Sigmoid或Tanh时，网络模型会发生梯度爆炸问题。而使用修正线性激活函数（ReLU）能够减少梯度爆炸发生的概率，对于隐藏层而言，使用修正线性激活函数（ReLU）是一个比较合适的激活函数，当然ReLU函数有许多变体，大家在实践过程中可以逐一使用以找到最合适的激活函数。

3.使用长短周期记忆网络

由于循环神经网络中存在的固有不稳定性，梯度爆炸可能会发生。比如，通过时间反向传播，其本质是将循环网络转变为深度多层感知神经网络。通过使用长短期记忆单元（LSTM）或相关的门控神经结构能够减少梯度爆炸发生的概率。对于循环神经网络的时间序列预测而言，采用LSTM是新的最佳实践。

4.使用梯度裁剪

在深度多层感知网络中，当有大批量数据以及LSTM是用于很长时间序列时，梯度爆炸仍然会发生。当梯度爆炸发生时，可以在网络训练时检查并限制梯度的大小，这被称作梯度裁剪。梯度裁剪是处理梯度爆炸问题的一个简单但非常有效的解决方案，如果梯度值大于某个阈值，我们就进行梯度裁剪。——自然语言处理中的神经网络方法的第5.2.4节具体而言，检查误差梯度值就是与一个阈值进行比较，若误差梯度值超过设定的阈值，则截断或设置为阈值。在某种程度上，梯度爆炸问题可以通过梯度裁剪来缓解（在执行梯度下降步骤之前对梯度进行阈值操作）——深度学习第294页在Keras深度学习库中，在训练网络之前，可以对优化器的clipnorm和  clipvalue参数进行设置来使用梯度裁剪，一般而言，默认将clipnorm和  clipvalue分别设置为1和0.5.在Keras API中使用优化器 5.使用权重正则化如果梯度爆炸问题仍然发生，另外一个方法是对网络权重的大小进行校验，并对大权重的损失函数增添一项惩罚项，这也被称作权重正则化，常用的有L1（权重的绝对值和）正则化与L2（权重的绝对值平方和再开方）正则化。使用L1或L2惩罚项会减少梯度爆炸的发生概率

作者：阿里云云栖社区
链接：https://zhuanlan.zhihu.com/p/32154263
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
